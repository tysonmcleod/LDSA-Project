{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAS: Project - Team 2\n",
    "Moritz Eck, moritz.eck.0055@student.uu.se<br>\n",
    "Tyson McLeod, <br>\n",
    "Isaline Baret, <br>\n",
    "Markella-Achilleia Zacharouli, <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start your application with dynamic allocation enabled, a timeout of no more than 30 seconds and a cap on CPU cores:\n",
    "# REMOTE SESSION\n",
    "# spark = SparkSession\\\n",
    "#        .builder\\\n",
    "#        .master(\"spark://192.168.1.153:7077\") \\\n",
    "#        .appName(\"LDSA_Team2_Project\")\\\n",
    "#        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "#        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "#        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "#        .config(\"spark.dynamicAllocation.maxExecutors\", 8)\\\n",
    "# #.config(\"spark.executor.instances\", 8)\\ # set this to 1 if you want to compare remote execution with local.\n",
    "#        .config(\"spark.executor.cores\",8)\\\n",
    "#        .config('spark.executor.memory', \"8g\")\\\n",
    "#        .config(\"spark.driver.cores\", 2)\\\n",
    "#        .config(\"spark.driver.memory\", \"2g\")\\\n",
    "#        .config(\"spark.executor.heartbeatInterval\",\"5s\")\\\n",
    "#        .getOrCreate()\n",
    "\n",
    "# LOCAL SESSION\n",
    "spark = SparkSession\\\n",
    "    .builder.master(\"local[4]\")\\\n",
    "    .appName(\"LDSA_Team2_Project\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.cores\",4)\\\n",
    "    .config('spark.executor.memory', \"8g\")\\\n",
    "    .config(\"spark.driver.cores\", 2)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\",\"5s\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark context (old RDD)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"INFO\")\n",
    "LOCAL = True # TODO: select if the experiment is run remote or local\n",
    "\n",
    "# filepaths\n",
    "REMOTE_HDFS_PATH = \"hdfs://192.168.1.153:9000/team02/input/\"\n",
    "LOCAL_PATH = \"./data/\"\n",
    "\n",
    "# filenames\n",
    "business_fn = \"yelp_academic_dataset_business.json\"\n",
    "users_fn = \"yelp_academic_dataset_users.json\"\n",
    "reviews_fn = \"yelp_academic_dataset_reviews.json\"\n",
    "\n",
    "# create the filepaths (remote or local)\n",
    "if LOCAL:\n",
    "    business_fp = LOCAL_PATH + business_fn\n",
    "    users_fp = LOCAL_PATH + users_fn\n",
    "    reviews_fp = LOCAL_PATH + reviews_fn\n",
    "else:\n",
    "    business_fp = REMOTE_HDFS_PATH + business_fn\n",
    "    users_fp = REMOTE_HDFS_PATH + users_fn\n",
    "    reviews_fp = REMOTE_HDFS_PATH + reviews_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Business Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read JSON file into PySpark dataframe\n",
    "business = spark.read.json(business_filepath)\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "business.printSchema()\n",
    "\n",
    "# show top 5 rows\n",
    "business.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of rows\n",
    "print(\"Rows in Business Dataframe:\\t\", business.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "print(\"Number of Partitions:\\t\\t\", business.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Sort all business according to stars and review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for i in range(10):\n",
    "    # top businesses according to starts and review_count\n",
    "    start_time = time.time()\n",
    "    filtered = business.filter(business.stars >= 4.0).sort(\"stars\", \"review_count\", ascending=[0,0]).head(10)\n",
    "    end_time = time.time()\n",
    "\n",
    "    for row in filtered:\n",
    "        name, stars, rc = row[\"name\"], row[\"stars\"], row[\"review_count\"]\n",
    "        print(\"Name:\\t{},\\tStars:\\t{},\\tReview Count:\\t{}\".format(name[:12], stars, rc))\n",
    "        \n",
    "    delta = end_time - start_time\n",
    "    times.append(delta)\n",
    "    print(\"\\nThe evaluation took: {} seconds\".format(delta))\n",
    "    \n",
    "    # remove the storage and space\n",
    "    del filtered, delta\n",
    "    \n",
    "print(\"Average Time Taken: {}\".format(sum(times)/len(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes the business dataframe (only do it if you need space)\n",
    "del business "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: User Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL \n",
    "# read JSON file\n",
    "users = spark.read.json(users_fp)\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "users.printSchema()\n",
    "\n",
    "# the number of rows\n",
    "print(\"Rows in Users Dataframe:\\t\", users.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "print(\"Number of Partitions:\\t\\t\", users.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# use udf to define a row-at-a-time udf\n",
    "def count_friends(line):\n",
    "    # lowercase transformation\n",
    "    # splitting into tokens/words\n",
    "    return len(line.lower().split(', '))\n",
    "\n",
    "# count the number of friends per user and add the value as a new column\n",
    "count_friends = udf(count_friends, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Sorting dataset according \"review_count\", \"useful\", \"fans\" and couting the number of friends per reviewer and sorting according to the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # sort according to reviewers with most reviews\n",
    "    top_reviewers = users.sort(\"review_count\", ascending=False).head(20)\n",
    "\n",
    "    # sort according to reviewers with most useful reviews\n",
    "    top_useful_reviews = users.sort(\"useful\", \"review_count\", ascending=[0,0]).head(20)\n",
    "\n",
    "    # sort according to reviewers with most fans\n",
    "    top_fan_count = users.sort(\"fans\", \"useful\", ascending=[0,0]).head(20)\n",
    "\n",
    "    # count the number of friends per reviewer\n",
    "    modified = users.withColumn(\"friendsCount\", count_friends(col(\"friends\")))\n",
    "\n",
    "    # sort according to reviewers with most friends and then fans\n",
    "    top_friends = modified.sort(\"friendsCount\", \"fans\", ascending=[0,0]).head(20)\n",
    "\n",
    "    end_time = time.time()\n",
    "    delta = end_time - start_time\n",
    "    times.append(delta)\n",
    "\n",
    "    print(\"Top 5 Reviewers by Review Count!\")\n",
    "    for row in top_reviewers[:5]:\n",
    "        name, since, rc = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"]\n",
    "        print(\"Name:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, rc, since))\n",
    "\n",
    "    print(\"\\nTop 5 Most Useful Reviews by Reviewer!\")\n",
    "    for row in top_useful_reviews[:5]:\n",
    "        name, since, rc, useful = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"], row[\"useful\"]\n",
    "        print(\"Name:\\t{}\\tUseful Reviews:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, useful, rc, since))\n",
    "\n",
    "    print(\"\\nTop 5 Reviewers with most Fans!\")\n",
    "    for row in top_fan_count[:5]:\n",
    "        name, since, rc, useful, fans = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"], row[\"useful\"], row[\"fans\"]\n",
    "        print(\"Name:\\t{}\\tFans:\\t{}\\tUseful Reviews:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, fans, useful, rc, since))\n",
    "\n",
    "    print(\"\\nTop 5 Reviewers with most Friends!\")\n",
    "    for row in top_friends[:5]:\n",
    "        name, since, fc, fans = row[\"name\"], row[\"yelping_since\"], row[\"friendsCount\"], row[\"fans\"]\n",
    "        print(\"Name:\\t{}\\tFriends:\\t{}\\tFans:\\t{}\\tYelping Since:\\t{}\".format(name, fans, fc, since))\n",
    "    \n",
    "    print(\"\\nThe evaluation took: {:3.3f} seconds\".format(end_time - start_time))\n",
    "    del top_reviewers, top_useful_reviews, top_fan_count, modified, top_friends, delta\n",
    "    \n",
    "print(\"Average Time: {}\".format(sum(times)/len(times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL \n",
    "# read JSON file\n",
    "reviews = spark.read.json(\"./data/reviews_1000000.json\")\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "reviews.printSchema()\n",
    "\n",
    "# the number of rows\n",
    "print(\"Rows in Reviews Dataframe:\\t\", reviews.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "print(\"Number of Partitions:\\t\\t\", reviews.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Preprocessing of Reviews & Join with Businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# use udf to define a row-at-a-time udf\n",
    "def preprocess(line):\n",
    "    # lowercase transformation\n",
    "    # splitting into tokens/words\n",
    "    tokens = line.lower().split(' ')\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    # TODO: do more fancy preprocessing (e.g., using NLTK or SpaCy to stem and remove stopwords)\n",
    "    return str(tokens)\n",
    "\n",
    "# tokenize preprocessing udf\n",
    "tok = udf(preprocess, StringType())\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# preprocess reviews\n",
    "pr_reviews = reviews.withColumn(\"text\", tok(col(\"text\")))\n",
    "pr_reviews.drop('user_id', 'review_id')\n",
    "\n",
    "# print first three rows\n",
    "# pr_reviews.show(5, False)\n",
    "\n",
    "# join business and reviews\n",
    "merged = business.join(pr_reviews, business.business_id == pr_reviews.business_id, 'left_outer').drop('attributes', 'hours')\n",
    "merged.show(3, False)\n",
    "\n",
    "end_time = time.time()\n",
    "    \n",
    "print(\"\\nThe evaluation took: {:3.3f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
