{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAS: Project - Team 2\n",
    "Moritz Eck, moritz.eck.0055@student.uu.se<br>\n",
    "Tyson McLeod, <br>\n",
    "Isaline Baret, <br>\n",
    "Markella-Achilleia Zacharouli, <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start your application with dynamic allocation enabled, a timeout of no more than 30 seconds and a cap on CPU cores:\n",
    "# REMOTE SESSION\n",
    "# spark = SparkSession\\\n",
    "#        .builder\\\n",
    "#        .master(\"spark://192.168.1.153:7077\") \\\n",
    "#        .appName(\"LDSA_Team2_Project\")\\\n",
    "#        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "#        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "#        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "#        .config(\"spark.dynamicAllocation.maxExecutors\", 8)\\\n",
    "# #.config(\"spark.executor.instances\", 8)\\ # set this to 1 if you want to compare remote execution with local.\n",
    "#        .config(\"spark.executor.cores\",8)\\\n",
    "#        .config('spark.executor.memory', \"8g\")\\\n",
    "#        .config(\"spark.driver.cores\", 2)\\\n",
    "#        .config(\"spark.driver.memory\", \"2g\")\\\n",
    "#        .config(\"spark.executor.heartbeatInterval\",\"5s\")\\\n",
    "#        .getOrCreate()\n",
    "\n",
    "# LOCAL SESSION\n",
    "spark = SparkSession\\\n",
    "    .builder.master(\"local[4]\")\\\n",
    "    .appName(\"LDSA_Team2_Project\")\\\n",
    "    .config(\"spark.executor.instances\", 1)\\\n",
    "    .config(\"spark.executor.cores\",4)\\\n",
    "    .config('spark.executor.memory', \"8g\")\\\n",
    "    .config(\"spark.driver.cores\", 2)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\",\"5s\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark context (old RDD)\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"INFO\")\n",
    "LOCAL = True # TODO: select if the experiment is run remote or local\n",
    "\n",
    "# filepaths\n",
    "REMOTE_HDFS_PATH = \"hdfs://192.168.1.153:9000/team02/input/\"\n",
    "LOCAL_PATH = \"../\"\n",
    "\n",
    "# filenames\n",
    "business_fn = \"yelp_academic_dataset_business.json\"\n",
    "users_fn = \"yelp_academic_dataset_user.json\"\n",
    "reviews_fn = \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "# create the filepaths (remote or local)\n",
    "if LOCAL:\n",
    "    business_fp = LOCAL_PATH + business_fn\n",
    "    users_fp = LOCAL_PATH + users_fn\n",
    "    reviews_fp = LOCAL_PATH + reviews_fn\n",
    "else:\n",
    "    business_fp = REMOTE_HDFS_PATH + business_fn\n",
    "    users_fp = REMOTE_HDFS_PATH + users_fn\n",
    "    reviews_fp = REMOTE_HDFS_PATH + reviews_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Business Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read JSON file into PySpark dataframe\n",
    "business = spark.read.json(business_fp)\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "#business.printSchema()\n",
    "\n",
    "# show top 5 rows\n",
    "#business.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of rows\n",
    "#print(\"Rows in Business Dataframe:\\t\", business.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "#print(\"Number of Partitions:\\t\\t\", business.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Sort all business according to stars and review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation took: 1.9836606979370117 seconds\n",
      "\n",
      "The evaluation took: 0.8734118938446045 seconds\n",
      "\n",
      "The evaluation took: 0.9278416633605957 seconds\n",
      "\n",
      "The evaluation took: 0.9600551128387451 seconds\n",
      "\n",
      "The evaluation took: 0.9410321712493896 seconds\n",
      "\n",
      "The evaluation took: 0.9506406784057617 seconds\n",
      "\n",
      "The evaluation took: 0.8186242580413818 seconds\n",
      "\n",
      "The evaluation took: 0.9506490230560303 seconds\n",
      "\n",
      "The evaluation took: 0.7654187679290771 seconds\n",
      "\n",
      "The evaluation took: 0.7846441268920898 seconds\n",
      "Average Time Taken: 0.9955978393554688\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for i in range(10):\n",
    "    # top businesses according to starts and review_count\n",
    "    start_time = time.time()\n",
    "    filtered = business.filter(business.stars >= 4.0).sort(\"stars\", \"review_count\", ascending=[0,0]).head(10)\n",
    "    end_time = time.time()\n",
    "\n",
    "    for row in filtered:\n",
    "        name, stars, rc = row[\"name\"], row[\"stars\"], row[\"review_count\"]\n",
    "        #print(\"Name:\\t{},\\tStars:\\t{},\\tReview Count:\\t{}\".format(name[:12], stars, rc))\n",
    "        \n",
    "    delta = end_time - start_time\n",
    "    times.append(delta)\n",
    "    print(\"\\nThe evaluation took: {} seconds\".format(delta))\n",
    "    \n",
    "    # remove the storage and space\n",
    "    del filtered, delta\n",
    "    \n",
    "print(\"Average Time Taken: {}\".format(sum(times)/len(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes the business dataframe (only do it if you need space)\n",
    "del business "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: User Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL \n",
    "# read JSON file\n",
    "users = spark.read.json(users_fp)\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "#users.printSchema()\n",
    "\n",
    "# the number of rows\n",
    "#print(\"Rows in Users Dataframe:\\t\", users.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "#print(\"Number of Partitions:\\t\\t\", users.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# use udf to define a row-at-a-time udf\n",
    "def count_friends(line):\n",
    "    # lowercase transformation\n",
    "    # splitting into tokens/words\n",
    "    return len(line.lower().split(', '))\n",
    "\n",
    "# count the number of friends per user and add the value as a new column\n",
    "count_friends = udf(count_friends, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Sorting dataset according \"review_count\", \"useful\", \"fans\" and couting the number of friends per reviewer and sorting according to the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation took: 36.356 seconds\n",
      "\n",
      "The evaluation took: 33.524 seconds\n",
      "\n",
      "The evaluation took: 33.138 seconds\n",
      "\n",
      "The evaluation took: 33.033 seconds\n",
      "\n",
      "The evaluation took: 33.494 seconds\n",
      "Average Time: 33.908977794647214\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # sort according to reviewers with most reviews\n",
    "    top_reviewers = users.sort(\"review_count\", ascending=False).head(20)\n",
    "\n",
    "    # sort according to reviewers with most useful reviews\n",
    "    top_useful_reviews = users.sort(\"useful\", \"review_count\", ascending=[0,0]).head(20)\n",
    "\n",
    "    # sort according to reviewers with most fans\n",
    "    top_fan_count = users.sort(\"fans\", \"useful\", ascending=[0,0]).head(20)\n",
    "\n",
    "    # count the number of friends per reviewer\n",
    "    modified = users.withColumn(\"friendsCount\", count_friends(col(\"friends\")))\n",
    "\n",
    "    # sort according to reviewers with most friends and then fans\n",
    "    top_friends = modified.sort(\"friendsCount\", \"fans\", ascending=[0,0]).head(20)\n",
    "\n",
    "    end_time = time.time()\n",
    "    delta = end_time - start_time\n",
    "    times.append(delta)\n",
    "\n",
    "    #print(\"Top 5 Reviewers by Review Count!\")\n",
    "    for row in top_reviewers[:5]:\n",
    "        name, since, rc = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"]\n",
    "        #print(\"Name:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, rc, since))\n",
    "\n",
    "    #print(\"\\nTop 5 Most Useful Reviews by Reviewer!\")\n",
    "    for row in top_useful_reviews[:5]:\n",
    "        name, since, rc, useful = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"], row[\"useful\"]\n",
    "        #print(\"Name:\\t{}\\tUseful Reviews:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, useful, rc, since))\n",
    "\n",
    "    #print(\"\\nTop 5 Reviewers with most Fans!\")\n",
    "    for row in top_fan_count[:5]:\n",
    "        name, since, rc, useful, fans = row[\"name\"], row[\"yelping_since\"], row[\"review_count\"], row[\"useful\"], row[\"fans\"]\n",
    "        #print(\"Name:\\t{}\\tFans:\\t{}\\tUseful Reviews:\\t{}\\tReview Count:\\t{}\\tYelping Since:\\t{}\".format(name, fans, useful, rc, since))\n",
    "\n",
    "    #print(\"\\nTop 5 Reviewers with most Friends!\")\n",
    "    for row in top_friends[:5]:\n",
    "        name, since, fc, fans = row[\"name\"], row[\"yelping_since\"], row[\"friendsCount\"], row[\"fans\"]\n",
    "        #print(\"Name:\\t{}\\tFriends:\\t{}\\tFans:\\t{}\\tYelping Since:\\t{}\".format(name, fans, fc, since))\n",
    "    \n",
    "    print(\"\\nThe evaluation took: {:3.3f} seconds\".format(end_time - start_time))\n",
    "    del top_reviewers, top_useful_reviews, top_fan_count, modified, top_friends, delta\n",
    "    \n",
    "print(\"Average Time: {}\".format(sum(times)/len(times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load User Data & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOCAL \n",
    "# read JSON file\n",
    "reviews = spark.read.json(reviews_fp)\n",
    "\n",
    "# the inferred schema can be visualized using the printSchema() method\n",
    "reviews.printSchema()\n",
    "\n",
    "# the number of rows\n",
    "#rint(\"Rows in Reviews Dataframe:\\t\", reviews.count())\n",
    "\n",
    "# the number of RDD partitions\n",
    "#rint(\"Number of Partitions:\\t\\t\", reviews.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Preprocessing of Reviews & Join with Businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation took: 3.760 seconds\n",
      "\n",
      "Average Time: 3.759615421295166\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re    \n",
    "\n",
    "# pre compile regex that match all non-ASCII characters\n",
    "pattern = re.compile(\"\\W\")\n",
    "\n",
    "# use udf to define a row-at-a-time udf\n",
    "def preprocess(line):\n",
    "    # lowercase transformation\n",
    "    # splitting into tokens/words\n",
    "    tokens = line.lower().split(' ')\n",
    "    tokens = [pattern.sub(\"\", token.strip()) for token in tokens]\n",
    "    return str(tokens)\n",
    "\n",
    "# tokenize preprocessing udf\n",
    "tok = udf(preprocess, StringType())\n",
    "\n",
    "times = []\n",
    "for i in range(1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # preprocess reviews\n",
    "    pr_reviews = reviews.withColumn(\"text\", tok(col(\"text\")))\n",
    "    pr_reviews = pr_reviews.drop('user_id', 'review_id')\n",
    "    pr_reviews = pr_reviews.withColumnRenamed(\"business_id\", \"bu_id\")\n",
    "\n",
    "    # print first three rows\n",
    "    # pr_reviews.show(5, False)\n",
    "\n",
    "    # join business and reviews\n",
    "    merged = business.join(pr_reviews, business.business_id == pr_reviews.bu_id, 'left_outer')\n",
    "\n",
    "    # drop duplicate column and filter out empty rows\n",
    "    merged = merged.drop(\"bu_id\").filter(merged.text.isNotNull())\n",
    "    result = merged.select([\"business_id\", \"categories\", \"text\"]).head(20)\n",
    "\n",
    "    end_time = time.time()\n",
    "    delta = end_time - start_time\n",
    "    times.append(delta)\n",
    "    \n",
    "    #print(result)\n",
    "\n",
    "    print(\"\\nThe evaluation took: {:3.3f} seconds\\n\".format(delta))\n",
    "    del pr_reviews, merged, result, delta\n",
    "    \n",
    "print(\"Average Time: {}\".format(sum(times)/len(times)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Preprocessing of Reviews & creation of Ngrams for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation took: 0.173 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf, regexp_replace, lower, split, size\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "# pre_process_text\n",
    "def pre_process_text(x):\n",
    "    x = lower(x)\n",
    "    x = regexp_replace(x, \"^rt \", \"\")\n",
    "    x = regexp_replace(x, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    return x \n",
    "\n",
    "\n",
    "# stemmer function\n",
    "def stem(input):\n",
    "    output = []\n",
    "    for x in input:\n",
    "        stemmy = stemmer.stem(x)\n",
    "        if(len(stemmy) > 2):\n",
    "            output.append(stemmy)\n",
    "    return output\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# pre_process_text df\n",
    "pre_process_text_df = reviews.select(pre_process_text(col(\"text\")).alias(\"text\"))\n",
    "pre_process_text_df.drop('user_id', 'review_id')\n",
    "\n",
    "\n",
    "# tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"vector\")\n",
    "vector_df = tokenizer.transform(pre_process_text_df).select(\"vector\")\n",
    "\n",
    "\n",
    "# remove stopwords \n",
    "stopword_remover = StopWordsRemover()\n",
    "stopwords = stopword_remover.getStopWords()\n",
    "stopword_remover.setInputCol(\"vector\")\n",
    "stopword_remover.setOutputCol(\"text_without_stopwords\")\n",
    "text_without_stopwords_df = stopword_remover.transform(vector_df).select(\"text_without_stopwords\")\n",
    "\n",
    "\n",
    "# stem tokens: greatest should become great, stopping -> stop etc.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# udf for stemming \n",
    "udf_stemmy = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# new dataframe with vectors containing stemmed tokens\n",
    "vector_with_stemmed_tokens = (\n",
    "    text_without_stopwords_df\n",
    "        .withColumn(\"text_stemmed\", udf_stemmy(\"text_without_stopwords\"))\n",
    "        .select(\"text_stemmed\")\n",
    "    )\n",
    "\n",
    "#vector_with_stemmed_tokens.printSchema()\n",
    "#vector_with_stemmed_tokens.show(4, False)\n",
    "# 12th row after a show(), walked around --> walk around after stem\n",
    "\n",
    "\n",
    "# create ngrams \n",
    "\n",
    "# x = suitable number for reviews?\n",
    "x = 3 \n",
    "\n",
    "ngram = NGram(n=x, inputCol= \"text_stemmed\", outputCol=\"ngrams\")\n",
    "vector_with_stemmed_tokens= ngram.transform(vector_with_stemmed_tokens)\n",
    "\n",
    "#vector_with_stemmed_tokens.printSchema()\n",
    "#vector_with_stemmed_tokens.show(1, False)\n",
    "\n",
    "ngrams_of_size_x = vector_with_stemmed_tokens.where(size(col(\"ngrams\")) >= x)\n",
    "\n",
    "#ngrams_of_size_x.printSchema()\n",
    "#ngrams_of_size_x.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThe evaluation took: {:3.3f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
